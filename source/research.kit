<!-- $subtitle: Research by -->
<!-- $description:
    A summary of my research.
-->
<!-- $research_active = active -->
<!-- @import "header.kit" -->

<span class="anchor" id="Interests"></span>
<h1>Research interests</h1>
<p>I am interested in human-like reasoning and the &ldquo;third wave&rdquo; of artificial intelligence. Humans are capable of finding patterns in remarkably small datasets, learning from just a handful of examples. We use a wide variety of strategies to solve a wide variety of problems. Truly intelligent systems should be able to do likewise. Third-wave AI focuses on systems that can not just make predictions, but form explanations. This goes far beyond the first-wave (<span class='all-caps'>GOFAI</span>, or good old fashioned AI) and second-wave (deep learning) AI. Furthermore, some speculate that third-wave AI might focus on teaching the AIs how to learn, or <em>meta-learning</em>.</p>


<hr />

<span class="anchor" id="Publications"></span>
<h1>Publications</h1>


<div class="dynamic-link">
    <h2>Inspection and Selection of Representations</h2>
    <span class="authors">Daniel Raggi, Aaron Stockdill, Mateja Jamnik, Grecia Garcia Garcia, Holly E. A. Sutherland, and Peter C.-H. Cheng</span>
    <div class='pub-info'>
        <span class='journal'>Intelligent Computer Mathematics</span>
        <span class="date pub-date">July 2019</span>
        <span class='orig'>The final publication is available at Springer via <a target='_blank' href='http://dx.doi.org/10.1007/978-3-030-23250-4_16'>http://dx.doi.org/10.1007/978-3-030-23250-4_16</a></span>
        <a class="pdflink" href="/static_assets/Inspection and Selection of Representations.pdf">Download PDF</a>
        <a class="bibtex" href="/static_assets/Inspection and Selection of Representations.bib">Download BibTeX</a>
    </div>
    <h3>Abstract</h3>
    <p class="abstract">We present a novel framework for inspecting representations and encoding their formal properties. This enables us to assess and compare the informational and cognitive value of different representations for reasoning. The purpose of our framework is to automate the process of representation selection, taking into account the candidate representation's match to the problem at hand and to the user's specific cognitive profile. This requires a language for talking about representations, and methods for analysing their relative advantages. This foundational work is first to devise a computational end-to-end framework where problems, representations, and user's profiles can be described and analysed. As AI systems become ubiquitous, it is important for them to be more compatible with human reasoning, and our framework enables just that.</p>
</div>
<div class="dynamic-link">
    <h2>Simulating neuromorphic reservoir computing: Abstract feed-forward hardware models</h2>
    <span class="authors">Aaron Stockdill and Kourosh Neshatian</span>
    <div class='pub-info'>
        <span class='journal'>2017 International Conference on Image and Vision Computing New Zealand (IVCNZ)</span>
        <span class="date pub-date">December 2016</span>
        <span class='orig'>The final publication is available through IEEEXplore via <a target='_blank' href='http://dx.doi.org/10.1109/IVCNZ.2017.8402482'>http://dx.doi.org/10.1109/IVCNZ.2017.8402482</a></span>
        <a class="pdflink" href="/static_assets/NeuromorphicReservoirs.pdf">Download PDF</a>
        <a class="bibtex" href="/static_assets/NeuromorphicReservoirs.bib">Download BibTeX</a>
    </div>
    <h3>Abstract</h3>
    <p class="abstract">Recent developments of unconventional hardware using memristors and atomic switch networks has led to renewed interest in hardware neuromorphic solutions. Most hardware models rely upon a reservoir neural network as the basis of any learning, but the distinct differences between software implementations and hardware reality mean what we take for granted in traditional software reservoirs - such as cycles, loops, infinite energy, and discrete time - may be severely limited or unavailable in hardware, raising questions about how a hardware implementation would perform and how to potentially overcome these limitations. Proposed hardware additions, such as an echoer or an input delay mechanism, address some of these limitations.</p>
</div>
<div class="dynamic-link">
    <h2>Restricted Echo State Networks</h2>
    <span class="authors">Aaron Stockdill and Kourosh Neshatian</span>
    <div class='pub-info'>
        <span class='journal'>AI 2016: Advances in Artificial Intelligence: 29th Australasian Joint Conference, Hobart, TAS, Australia, December 5-8, 2016, Proceedings</span>
        <span class="date pub-date">December 2016</span>
        <span class='orig'>The final publication is available at Springer via <a target='_blank' href='http://dx.doi.org/10.1007/978-3-319-50127-7_49'>http://dx.doi.org/10.1007/978-3-319-50127-7_49</a></span>
        <a class="pdflink" href="/static_assets/Restricted Echo State Networks.pdf">Download PDF</a>
        <a class="bibtex" href="/static_assets/Restricted Echo State Networks.bib">Download BibTeX</a>
    </div>
    <h3>Abstract</h3>
    <p class="abstract">Echo state networks are a powerful type of reservoir neural network, but the reservoir is essentially unrestricted in its original formulation. Motivated by limitations in neuromorphic hardware, we remove combinations of the four sources of memory—leaking, loops, cycles, and discrete time—to determine how these influence the suitability of the reservoir. We show that loops and cycles can replicate each other, while discrete time is a necessity. The potential limitation of energy conservation is equivalent to limiting the spectral radius.</p>
</div>
<div class="dynamic-link">
    <h2>Neuromorphic Computing with Reservoir Neural Networks on Memristive Hardware</h2>
    <span class="authors">Aaron Stockdill</span>
    <div class='pub-info'>
        
        <span class="date pub-date">October 2016</span>
        
        <a class="pdflink" href="/static_assets/Neuromorphic Computing.pdf">Download PDF</a>
        <a class="bibtex" href="/static_assets/Neuromorphic Computing.bib">Download BibTeX</a>
    </div>
    <h3>Abstract</h3>
    <p class="abstract">Building an artificial brain is a goal as old as computer science. Neuromorphic computing takes this in new directions by attempting to physically simulate the human brain. In 2008 this goal received renewed interest due to the memristor, a resistor that has state, and again in 2012 with the atomic switch, a related circuit component. This report details the construction of a simulator for large networks of these devices, including the underlying assumptions and how we model specific physical characteristics. Existing simulations of neuromorphic hardware range from detailed particle-level simulations through to high-level graph-theoretic representations. We develop a simulator that sits in the middle, successfully removing expensive and unnecessary operations from particle simulators while remaining more device-accurate than a wholly abstract representation. We achieve this with a statistical approach, describing distributions from which we draw the ideal values based on a small set of parameters. This report also explores the applications of these memristive networks in machine learning using reservoir neural networks, and their performance in comparison to existing techniques such as echo state networks (ESNs). Neither the memristor nor atomic switch networks are capable of learning time-series sequences, and the underlying cause is found to be restrictions imposed by physical laws upon circuits. We present a series of restrictions upon an ESN, systematically removing loops, cycles, discrete time, and combinations of these three factors. From this we conclude that removing loops and cycles breaks the “infinite memory” of an ESN, and removing all three renders the reservoir totally incapable of learning.</p>
</div>


<hr />


<span class="anchor" id="Talks"></span>
<h1>Talks</h1>

<div class="dynamic-link"><a href="/talk/selwyn-postgraduate-seminar/">Selwyn Postgraduate Seminar</a><span class="date talk-date">2018-05-01</span></div><div class="dynamic-link"><a href="/talk/csx/">CS+X</a><span class="date talk-date">2017-08-18</span></div><div class="dynamic-link"><a href="/talk/life-after-the-robot-apocalypse/">Life After the Robot Apocalypse</a><span class="date talk-date">2016-09-08</span></div><div class="dynamic-link"><a href="/talk/neuromorphic-computing-with-reservoir-neural-networks-on-memristive-hardware/">Neuromorphic Computing with Reservoir Neural Networks on Memristive Hardware</a><span class="date talk-date">2016-09-01</span></div>

<hr />


<span class="anchor" id="GitHub"></span>
<h1>GitHub</h1>
<p>I infrequently put projects on GitHub, but you are welcome to view what is available there: <a href="https://github.com/aaronstockdill"> Aaron Stockdill on GitHub</a>.</p>

<!-- @import "footer.kit" -->
