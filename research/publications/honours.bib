@misc{stockdill16b,
	Author = {Stockdill, Aaron},
	Month = {October},
	Title = {Neuromorphic Computing with Reservoir Neural Networks on Memristive Hardware},
	Year = {2016},
	Note = {Honours Report},
    Abstract = {Building an artificial brain is a goal as old as computer science. Neuromorphic computing takes this in new directions by attempting to physically simulate the human brain. In 2008 this goal received renewed interest due to the memristor, a resistor that has state, and again in 2012 with the atomic switch, a related circuit component. This report details the construction of a simulator for large networks of these devices, including the underlying assumptions and how we model specific physical characteristics. Existing simulations of neuromorphic hardware range from detailed particle-level simulations through to high-level graph-theoretic representations. We develop a simulator that sits in the middle, successfully removing expensive and unnecessary operations from particle simulators while remaining more device-accurate than a wholly abstract representation. We achieve this with a statistical approach, describing distributions from which we draw the ideal values based on a small set of parameters. This report also explores the applications of these memristive networks in machine learning using reservoir neural networks, and their performance in comparison to existing techniques such as echo state networks (ESNs). Neither the memristor nor atomic switch networks are capable of learning time-series sequences, and the underlying cause is found to be restrictions imposed by physical laws upon circuits. We present a series of restrictions upon an ESN, systematically removing loops, cycles, discrete time, and combinations of these three factors. From this we conclude that removing loops and cycles breaks the “infinite memory” of an ESN, and removing all three renders the reservoir totally incapable of learning.}}
